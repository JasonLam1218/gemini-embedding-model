#!/usr/bin/env python3

"""
Main pipeline controller for embedding-based exam generation system.
"""

import sys
import os
import click
from loguru import logger
from pathlib import Path
import json
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Add project root to Python path
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

# Import project modules
from config.settings import BATCH_SIZE
from src.core.text.text_loader import TextLoader
from src.core.text.chunker import TextChunker
from src.core.embedding.embedding_generator import EmbeddingGenerator
from src.core.generation.structure_generator import StructureGenerator
from src.core.storage.vector_store import VectorStore

@click.group()
def cli():
    """Embedding-Based Exam Generation Pipeline CLI"""
    pass

@cli.command()
@click.option('--input-dir', default='data/input/kelvin_papers', help='Input directory')
def process_texts(input_dir):
    """Load and process text files into chunks"""
    try:
        # Ensure output directory exists
        output_dir = Path("data/output/processed")
        output_dir.mkdir(parents=True, exist_ok=True)

        loader = TextLoader()
        documents = loader.process_directory(Path(input_dir))

        # Process documents into chunks
        chunker = TextChunker()
        all_chunks = []
        
        for doc in documents:
            chunks = chunker.chunk_text(doc.content)
            for i, chunk in enumerate(chunks):
                chunk_data = {
                    "id": f"{doc.paper_set}_{doc.paper_number}_{i}",
                    "chunk_text": chunk,
                    "chunk_index": i,
                    "chunk_size": len(chunk),
                    "source_file": doc.source_file,
                    "paper_set": doc.paper_set,
                    "paper_number": doc.paper_number,
                    "metadata": doc.metadata
                }
                all_chunks.append(chunk_data)

        # Save processed chunks
        chunks_path = output_dir / "processed_chunks.json"
        with open(chunks_path, "w", encoding="utf-8") as f:
            json.dump(all_chunks, f, indent=2, ensure_ascii=False)

        # Save processed documents
        docs_path = output_dir / "processed_documents.json"
        loader.save_processed_documents(docs_path)

        logger.info(f"âœ… Text processing completed. Processed {len(documents)} documents into {len(all_chunks)} chunks")
        logger.info(f"âœ… Chunks saved to: {chunks_path}")
        logger.info(f"âœ… Documents saved to: {docs_path}")

    except Exception as e:
        logger.error(f"âŒ Text processing failed: {e}")
        raise


@cli.command()
def generate_embeddings():
    """Generate embeddings for all processed chunks using Gemini API"""
    try:
        chunks_path = Path("data/output/processed/processed_chunks.json")
        if not chunks_path.exists():
            logger.error("âŒ No processed chunks found. Run 'process-texts' first.")
            return

        with open(chunks_path, "r", encoding="utf-8") as f:
            chunks = json.load(f)

        logger.info(f"ğŸ“ Generating embeddings for {len(chunks)} chunks")

        generator = EmbeddingGenerator()
        
        # Generate embeddings with metadata
        embeddings_data = []
        for chunk in chunks:
            try:
                embedding = generator.generate_single_embedding(chunk["chunk_text"])
                if embedding:
                    chunk_with_embedding = {
                        **chunk,
                        "embedding": embedding,
                        "embedding_model": "text-embedding-004"
                    }
                    embeddings_data.append(chunk_with_embedding)
                    logger.info(f"âœ… Generated embedding for chunk {chunk['id']}")
            except Exception as e:
                logger.error(f"âŒ Failed to generate embedding for chunk {chunk['id']}: {e}")
                continue

        # Save embeddings
        output_path = Path("data/output/processed/embeddings.json")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(embeddings_data, f, indent=2, ensure_ascii=False)

        logger.info(f"âœ… Embedding generation completed. Generated {len(embeddings_data)} embeddings")
        logger.info(f"âœ… Results saved to: {output_path}")

    except Exception as e:
        logger.error(f"âŒ Embedding generation failed: {e}")
        raise


@cli.command()
@click.option('--topic', default='AI and Data Analytics', help='Exam topic')
@click.option('--structure-type', default='standard', help='Exam structure type')
def generate_structured_exam(topic, structure_type):
    """Generate structured exam paper using embedding similarity"""
    try:
        # Check if embeddings exist
        embeddings_path = Path("data/output/processed/embeddings.json")
        if not embeddings_path.exists():
            logger.error("âŒ No embeddings found. Run 'generate-embeddings' first.")
            return

        logger.info(f"ğŸ”„ Generating structured exam paper for topic: {topic}")
        
        # Initialize structure generator
        structure_gen = StructureGenerator()
        
        # Generate exam with 4 main questions and sub-parts
        exam_paper = structure_gen.generate_structured_exam(
            topic=topic,
            structure_type=structure_type,
            num_main_questions=4
        )

        # Save generated exam
        output_dir = Path("data/output/generated_exams")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"structured_exam_{timestamp}.json"

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(exam_paper, f, indent=2, ensure_ascii=False)

        logger.info(f"âœ… Generated structured exam paper")
        logger.info(f"âœ… Exam saved to: {output_file}")

        # Also save as formatted text
        formatted_file = output_dir / f"structured_exam_{timestamp}.txt"
        structure_gen.save_formatted_exam(exam_paper, formatted_file)
        logger.info(f"âœ… Formatted exam saved to: {formatted_file}")

    except Exception as e:
        logger.error(f"âŒ Structured exam generation failed: {e}")
        raise

@cli.command()
def run_full_pipeline():
    """Run the complete embedding-based exam generation pipeline"""
    try:
        click.echo("ğŸš€ Starting full embedding-based exam generation pipeline...")
        ctx = click.get_current_context()

        click.echo("ğŸ“ Step 1: Processing text inputs...")
        ctx.invoke(process_texts)

        click.echo("ğŸ§  Step 2: Generating embeddings using Gemini API...")
        ctx.invoke(generate_embeddings)

        click.echo("ğŸ“‹ Step 3: Generating structured exam paper...")
        ctx.invoke(generate_structured_exam)

        click.echo("âœ… Pipeline completed successfully!")

    except Exception as e:
        click.echo(f"âŒ Pipeline failed: {e}")
        raise

@cli.command()
def status():
    """Show pipeline status and embedding information"""
    click.echo("ğŸ“Š Embedding-Based Pipeline Status Report")
    click.echo("=" * 50)

    # Check processed chunks
    chunks_file = Path("data/output/processed/processed_chunks.json")
    if chunks_file.exists():
        with open(chunks_file) as f:
            chunks = json.load(f)
        click.echo(f"ğŸ“„ Processed chunks: {len(chunks)}")
    else:
        click.echo("ğŸ“„ Processed chunks: Not found")

    # Check embeddings
    embeddings_file = Path("data/output/processed/embeddings.json")
    if embeddings_file.exists():
        with open(embeddings_file) as f:
            embeddings = json.load(f)
        click.echo(f"ğŸ§  Generated embeddings: {len(embeddings)}")
        if embeddings:
            click.echo(f"ğŸ§  Embedding dimensions: {len(embeddings[0].get('embedding', []))}")
    else:
        click.echo("ğŸ§  Generated embeddings: Not found")

    # Check generated exams
    exams_dir = Path("data/output/generated_exams")
    if exams_dir.exists():
        exam_files = list(exams_dir.glob("*.json"))
        click.echo(f"ğŸ“‹ Generated exams: {len(exam_files)}")
    else:
        click.echo("ğŸ“‹ Generated exams: Not found")

if __name__ == '__main__':
    cli()
